{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from huggingface_hub import notebook_login\n",
    "from transformers import AutoModelForSequenceClassification,AutoTokenizer, AutoConfig\n",
    "from transformers import TrainingArguments,Trainer\n",
    "from datasets import Dataset,DatasetDict\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import kaggle, zipfile\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'nlp-getting-started'\n",
    "df = pd.read_csv(path + '/train.csv')\n",
    "df.drop(columns = ['keyword', 'location'], inplace = True)\n",
    "# df['target'] = df['target'].astype(float)\n",
    "ds = Dataset.from_pandas(df)\n",
    "model_nm = 'microsoft/deberta-v3-small'\n",
    "tokz = AutoTokenizer.from_pretrained(model_nm)\n",
    "def tok_func(x): return tokz(x[\"text\"])\n",
    "tok_ds = ds.map(tok_func, batched=True)\n",
    "tok_ds = tok_ds.rename_columns({'target':'labels'})\n",
    "dds = tok_ds.train_test_split(0.25, seed=42)\n",
    "eval_df = pd.read_csv(path + '/test.csv')\n",
    "eval_df.drop(columns = ['keyword', 'location'], inplace = True)\n",
    "#eval_df['target'] = df['target'].astype(float)\n",
    "eval_ds = Dataset.from_pandas(eval_df).map(tok_func, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the mappings as dictionaries\n",
    "label2id = {\"Chance of being a disaster\": 0}\n",
    "id2label = {\"0\": \"Chance of being a disaster\"}\n",
    "# define model checkpoint - can be the same model that you already have on the hub\n",
    "model_ckpt = 'models/vnsrz/outputs'\n",
    "# define config\n",
    "config = AutoConfig.from_pretrained(model_ckpt, label2id=label2id, id2label=id2label)\n",
    "# load model with config\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_ckpt, config=config)\n",
    "# export model\n",
    "model.save_pretrained(\"model/twt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    metric = evaluate.load('f1')\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.clip(logits, 0, 1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "bs = 64\n",
    "epochs = 4\n",
    "lr = 8e-5\n",
    "\n",
    "args = TrainingArguments(\n",
    "    'outputs', \n",
    "    learning_rate=lr, \n",
    "    warmup_ratio=0.1, \n",
    "    lr_scheduler_type='cosine', \n",
    "    fp16=True, \n",
    "    evaluation_strategy=\"epoch\", \n",
    "    per_device_train_batch_size=bs, \n",
    "    per_device_eval_batch_size=bs*2, \n",
    "    num_train_epochs=epochs, \n",
    "    weight_decay=0.01, \n",
    "    report_to='none'\n",
    "    )\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_nm, num_labels=1)\n",
    "trainer = Trainer(\n",
    "    model, \n",
    "    args, \n",
    "    train_dataset=dds['train'], \n",
    "    eval_dataset=dds['test'],\n",
    "    tokenizer=tokz, \n",
    "    compute_metrics=compute_metrics\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 | packaged by conda-forge | (main, Nov 22 2022, 08:26:04) [GCC 10.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8c0a290cf5fa2218d12f3d3291b5f72e2163085b8314dd6e64f4de46ba9cdb91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
